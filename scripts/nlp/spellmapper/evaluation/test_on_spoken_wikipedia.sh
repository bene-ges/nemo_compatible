#!/bin/bash

# Copyright (c) 2023, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


NEMO_PATH=/home/aleksandraa/nemo

## Spellchecking model in nemo format, that you get after training. See run_training.sh or run_training_tarred.sh  
PRETRAINED_MODEL=training.nemo

## These two files are generated by dataset_preparation/get_ngram_mappings.sh 
NGRAM_MAPPINGS=replacement_vocab_filt.txt
SUB_MISSPELLS=sub_misspells.txt

DATA_DIR="/home/aleksandraa/data/spoken_wikipedia"
INPUT_DIR=${DATA_DIR}/english_prepared
OUTPUT_DIR=${DATA_DIR}/english_result


## How to prepare Spoken Wikipedia corpus.
##   Use
##   ${NEMO_PATH}/scripts/dataset_processing/spoken_wikipedia/run.sh
##   but replace in that bash-script
##   ```
##      mkdir ${INPUT_DIR}_prepared/audio
##      mkdir ${INPUT_DIR}_prepared/text
##      python ${NEMO_PATH}/scripts/dataset_processing/spoken_wikipedia/preprocess.py --input_folder ${INPUT_DIR} --destination_folder ${INPUT_DIR}_prepared
##   ```
##   with
##   ```
##      mkdir ${INPUT_DIR}_prepared/audio
##      mkdir ${INPUT_DIR}_prepared/text
##      mkdir ${INPUT_DIR}_prepared/vocabs
##      python ${NEMO_PATH}/examples/nlp/spellchecking_asr_customization/evaluation/preprocess_spoken_wikipedia_and_create_vocabs.py --input_folder ${INPUT_DIR} --destination_folder ${INPUT_DIR}_prepared
##   ```
##   It does the same preprocessing but also creates vocabs folder with some files to be used later.

## At this moment your ${DATA_DIR} folder structure should look like this:
##  data_folder_example
##   ├── english
##   |   ├── (I_Can%27t_Get_No)_Satisfaction
##   |   ├── ...
##   │   ├── Zinc
##   │   └── Zorbing
##   ├── english_prepared
##   │   ├── audio
##   |   |   ├── 1.ogg
##   |   |   ├── ...
##   |   |   └── 1340.ogg
##   │   ├── text
##   |   |   ├── 1.txt
##   |   |   ├── ...
##   |   |   └── 1340.txt
##   │   └── vocabs
##   |       ├── 1.headings.txt
##   |       ├── ...
##   |       ├── 1340.headings.txt
##   |       └── idf.txt
##   └── english_result
##       ├── clips
##       ├── manifests
##       |   ├── manifest.json
##       |   └── manifest_transcribed_metrics_filtered.json
##       ├── processed
##       |   ├── ...
##       |   ├── 1000.txt
##       |   ├── 1000.wav
##       |   ├── 1000_with_punct.txt
##       |   ├── 1000_with_punct_normalized.txt
##       |   ├── ...
##       |   └── en_grammars
##       ├── segments
##       └── verified_segments

## manifest_transcribed_metrics_filtered.json already contains transcriptions by ASR baseline model, specified in ${NEMO_PATH}/scripts/dataset_processing/spoken_wikipedia/run.sh 
## Note that some output files can be missing or empty, because of unsuccessful ctc-segmentation - this is ok.

## Create custom vocabularies in ${INPUT_DIR}/vocabs/{1..1340}.custom.txt
python ${NEMO_PATH}/examples/nlp/spellchecking_asr_customization/evaluation/create_custom_vocabs.py --folder ${INPUT_DIR} --processed_folder ${OUTPUT_DIR}/processed --min_len 6

## Split ASR output transcriptions into shorter fragments to serve as ASR hypotheses for spellchecking model
mkdir ${OUTPUT_DIR}/hypotheses
python ${NEMO_PATH}/examples/nlp/spellchecking_asr_customization/evaluation/extract_asr_hypotheses.py \
  --manifest ${OUTPUT_DIR}/manifests/manifest_transcribed_metrics_filtered.json \
  --folder ${OUTPUT_DIR}/hypotheses

## Prepare inputs for inference of neural customization spellchecking model
mkdir ${OUTPUT_DIR}/spellchecker_input
mkdir ${OUTPUT_DIR}/spellchecker_output
python ${NEMO_PATH}/examples/nlp/spellchecking_asr_customization/evaluation/prepare_input_for_spellchecker_inference.py \
  --hypotheses_folder ${DATA_DIR}/english_result/hypotheses \
  --vocabs_folder ${DATA_DIR}/english_prepared/vocabs \
  --output_folder ${DATA_DIR}/english_result/spellchecker_input \
  --ngram_mapping ${NGRAM_MAPPINGS} \
  --sub_misspells_file ${SUB_MISSPELLS}

## Create filelist with input filenames
rm ${DATA_DIR}/filelist.txt
for i in {1..1341}
do
    echo ${DATA_DIR}/english_result/spellchecker_input/${i}.txt >> ${DATA_DIR}/filelist.txt
done

## Run inference with neural customization spellchecking model
python ${NEMO_PATH}/examples/nlp/spellchecking_asr_customization/spellchecking_asr_customization_infer.py \
  pretrained_model=${PRETRAINED_MODEL} \
  model.max_sequence_len=512 \
  +inference.from_filelist=${DATA_DIR}/filelist.txt \
  +inference.output_folder=${OUTPUT_DIR}/spellchecker_output \
  inference.batch_size=16 \
  lang=en

## Postprocess and combine spellchecker results into a single manifest
python ${NEMO_PATH}/examples/nlp/spellchecking_asr_customization/evaluation/update_transcription_with_spellchecker_results.py \
  --asr_hypotheses_folder ${OUTPUT_DIR}/hypotheses \
  --spellchecker_inputs_folder ${OUTPUT_DIR}/spellchecker_input \
  --spellchecker_results_folder ${OUTPUT_DIR}/spellchecker_output \
  --input_manifest ${OUTPUT_DIR}/manifests/manifest_transcribed_metrics_filtered.json \
  --output_manifest ${OUTPUT_DIR}/manifests/manifest_corrected.json \
  --min_cov 0.4 \
  --min_real_cov 0.8 \
  --min_dp_score_per_symbol -1.5 \
  --ngram_mappings ${NGRAM_MAPPINGS}

## Check CER of spellchecker results
python ${NEMO_PATH}/examples/asr/speech_to_text_eval.py \
  dataset_manifest=${OUTPUT_DIR}/manifests/manifest_corrected.json \
  use_cer=True \
  only_score_manifest=True

## Check WER of spellchecker results
python ${NEMO_PATH}/examples/asr/speech_to_text_eval.py \
  dataset_manifest=${OUTPUT_DIR}/manifests/manifest_corrected.json \
  use_cer=False \
  only_score_manifest=True

## Check CER of baseline ASR results
python ${NEMO_PATH}/examples/asr/speech_to_text_eval.py \
  dataset_manifest=${OUTPUT_DIR}/manifests/manifest_transcribed_metrics_filtered.json \
  use_cer=True \
  only_score_manifest=True

## Check WER of baseline ASR results
python ${NEMO_PATH}/examples/asr/speech_to_text_eval.py \
  dataset_manifest=${OUTPUT_DIR}/manifests/manifest_transcribed_metrics_filtered.json \
  use_cer=False \
  only_score_manifest=True

## Perform error analysis and create "ideal" spellchecker results for comparison
python ${NEMO_PATH}/examples/nlp/spellchecking_asr_customization/evaluation/analyze_custom_ref_vs_asr.py \
  --manifest ${OUTPUT_DIR}/manifests/manifest_corrected.json \
  --vocab_dir ${INPUT_DIR}/vocabs \
  --input_dir ${OUTPUT_DIR}/spellchecker_input \
  --ngram_mappings ${NGRAM_MAPPINGS} \
  --output_name ${OUTPUT_DIR}/analysis_ref_vs_asr.txt

## Check CER of "ideal" spellcheck results
python ${NEMO_PATH}/examples/asr/speech_to_text_eval.py \
  dataset_manifest=${OUTPUT_DIR}/analysis_ref_vs_asr.txt.ideal_spellcheck \
  use_cer=True \
  only_score_manifest=True

## Check WER of "ideal" spellcheck results
python ${NEMO_PATH}/examples/asr/speech_to_text_eval.py \
  dataset_manifest=${OUTPUT_DIR}/analysis_ref_vs_asr.txt.ideal_spellcheck \
  use_cer=False \
  only_score_manifest=True
